{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.debugger import set_trace\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming the dataset was downloaded from Kaggle, tar file unzipped,\n",
    "# and is on your Desktop in a folder called archive\n",
    "\n",
    "# Set up the home directory \n",
    "home_directory = os.path.expanduser(\"~\")\n",
    "\n",
    "# File paths for the dataset\n",
    "train_file_path = os.path.join(home_directory, 'Desktop/archive/sign_mnist_train.csv')\n",
    "test_file_path = os.path.join(home_directory, 'Desktop/archive/sign_mnist_test.csv')\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv(train_file_path)\n",
    "test_df = pd.read_csv(test_file_path)\n",
    "\n",
    "# Vectorize data, and  normalize\n",
    "\n",
    "x_train = train_df.iloc[:, 1:].values / 255.0\n",
    "x_test = test_df.iloc[:, 1:].values / 255.0\n",
    "\n",
    "# Reshape the pixel values into flat vectors\n",
    "x_train_mlp = x_train.reshape(-1, 28*28)\n",
    "x_test_mlp = x_test.reshape(-1, 28*28)\n",
    "\n",
    "\n",
    "# Now separate our dataframes into inputs (features) and outputs (labels)\n",
    "y_train = train_df['label'].values  # Convert to numpy array\n",
    "y_test = test_df['label'].values    # Convert to numpy array\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "num_classes = 26  # labels 0-25 classes \n",
    "y_train_encoded = np.eye(num_classes)[y_train]\n",
    "y_test_encoded = np.eye(num_classes)[y_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class NeuralNetLayer:\n",
    "    def __init__(self):\n",
    "        self.gradient = None\n",
    "        self.parameters = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class LinearLayer(NeuralNetLayer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.ni = input_size\n",
    "        self.no = output_size\n",
    "        self.w = np.random.randn(output_size, input_size)\n",
    "        self.b = np.random.randn(output_size)\n",
    "        self.cur_input = None\n",
    "        self.parameters = [self.w, self.b]\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.cur_input = x\n",
    "        return (self.w[None, :, :] @ x[:, :, None]).squeeze() + self.b\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        assert self.cur_input is not None, \"Must call forward before backward\"\n",
    "        #dw = gradient.dot(self.cur_input)\n",
    "        dw = gradient[:, :, None] @ self.cur_input[:, None, :]\n",
    "        db = gradient\n",
    "        self.gradient = [dw, db]\n",
    "        return gradient.dot(self.w)\n",
    "    \n",
    "class ReLULayer(NeuralNetLayer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.gradient = np.where(x > 0, 1.0, 0.0)\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        assert self.gradient is not None, \"Must call forward before backward\"\n",
    "        return gradient * self.gradient\n",
    "    \n",
    "\n",
    "class SoftmaxOutputLayer(NeuralNetLayer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cur_probs = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        exps = np.exp(x)\n",
    "        probs = exps / np.sum(exps, axis=-1)[:, None]\n",
    "        self.cur_probs = probs\n",
    "        return probs\n",
    "\n",
    "    def backward(self, target):\n",
    "        assert self.cur_probs is not None, \"Must call forward before backward\"\n",
    "        return self.cur_probs - target\n",
    "    \n",
    "class MLP:\n",
    "    def __init__(self, activation_function, hidden_layer_sizes):\n",
    "        self.layers = []\n",
    "        input_size = 28 * 28  # Assuming MNIST-like input size\n",
    "        for output_size in hidden_layer_sizes:\n",
    "            self.layers.append(LinearLayer(input_size, output_size))\n",
    "            self.layers.append(activation_function())\n",
    "            input_size = output_size\n",
    "        self.layers.append(LinearLayer(input_size, 26))  # Output layer\n",
    "        self.layers.append(SoftmaxOutputLayer())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, target):\n",
    "        for layer in self.layers[::-1]:\n",
    "            target = layer.backward(target)\n",
    "\n",
    "    def fit(self, X, y, learning_rate, num_iterations):\n",
    "        labels = np.eye(26)[np.array(y)]\n",
    "        for _ in tqdm(range(num_iterations)):\n",
    "            predictions = self.forward(X)\n",
    "            loss = -(labels * np.log(predictions)).sum(axis=-1).mean()\n",
    "            self.backward(labels)\n",
    "            self.update_weights(learning_rate)\n",
    "        return loss\n",
    "\n",
    "    def update_weights(self, learning_rate):\n",
    "        for layer in self.layers:\n",
    "            if layer.parameters is not None:\n",
    "                for param, grad in zip(layer.parameters, layer.gradient):\n",
    "                    param -= learning_rate * grad.mean(axis=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = self.forward(X)\n",
    "        return np.argmax(predictions, axis=1)\n",
    "\n",
    "\n",
    "def evaluate_acc(self,y, y_pred):\n",
    "        accuracy = sum(y_pred.argmax(axis=1) == y.argmax(axis=1))\n",
    "        accuracy = accuracy / y.shape[0]\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Initialize MLP\n",
    "mlp = MLP(activation_function=ReLULayer, hidden_layer_sizes=[64, 64])\n",
    "\n",
    "# Train MLP\n",
    "mlp.fit(x_train, y_train, learning_rate=0.1, num_iterations=200)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = mlp.predict(x_train)\n",
    "y_pred_test = mlp.predict(x_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_train = evaluate_acc(y_train, y_pred_train)\n",
    "accuracy_test = evaluate_acc(y_test, y_pred_test)\n",
    "\n",
    "print(\"Training Accuracy:\", accuracy_train)\n",
    "print(\"Test Accuracy:\", accuracy_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
