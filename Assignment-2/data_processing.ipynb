{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from LinearRegression import LinearRegression\n",
    "from LogisticRegression import CustomLogisticRegression\n",
    "from MultiClassRegression import MultiClassRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.stats import gaussian_kde \n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extact, Transform, Load (ETL) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_vocab(vocab_file_path):\n",
    "    \"\"\"\n",
    "    Loads the vocabulary from a file and creates two dictionaries: \n",
    "    one mapping indices to words and another mapping words to indices.\n",
    "    \"\"\"\n",
    "    index_to_feature_map = {}\n",
    "    with open(vocab_file_path, 'r', encoding='utf-8') as vocab_file:\n",
    "        for index, word in enumerate(vocab_file):\n",
    "            index_to_feature_map[str(index)] = word.strip()\n",
    "    feature_to_index_map = {word: index for index, word in index_to_feature_map.items()}\n",
    "    return index_to_feature_map, feature_to_index_map\n",
    "\n",
    "def parse_review_features(line):\n",
    "    features = line.strip().split()[1:]  # Skip the first element (label)\n",
    "    return [feature.split(':') for feature in features]\n",
    "\n",
    "def compute_review_frequencies(feature_path):\n",
    "    \"\"\"\n",
    "    Computes the frequency of each word across reviews.\n",
    "    \"\"\"\n",
    "    review_frequencies = defaultdict(int)\n",
    "    total_reviews = 0\n",
    "    with open(feature_path, 'r', encoding='utf-8') as feat_file:\n",
    "        for line in feat_file:\n",
    "            total_reviews += 1\n",
    "            for index, _ in parse_review_features(line):\n",
    "                review_frequencies[index] += 1\n",
    "    return review_frequencies, total_reviews\n",
    "\n",
    "def filter_feature_indices(review_frequencies, total_reviews, lower_bound=0.01, upper_bound=0.5):\n",
    "    \"\"\"\n",
    "    Filters out feature indices based on review frequency criteria.\n",
    "    \"\"\"\n",
    "    min_reviews = total_reviews * lower_bound\n",
    "    max_reviews = total_reviews * upper_bound\n",
    "    filtered_feature_indices = {index for index, freq in review_frequencies.items() \n",
    "                                if min_reviews <= freq <= max_reviews}\n",
    "\n",
    "    return sorted([int(i) for i in filtered_feature_indices])\n",
    "\n",
    "def load_review_features(feature_path, index_mapping, n_reviews, n_features):\n",
    "    \"\"\"\n",
    "    Loads review features and labels into matrices based on a filtered and remapped vocabulary.\n",
    "    \"\"\"\n",
    "    X = np.zeros((n_reviews, n_features))\n",
    "    y = np.zeros(n_reviews)\n",
    "    with open(feature_path, 'r') as file:\n",
    "        for review_id, line in enumerate(file):\n",
    "            parts = line.strip().split()\n",
    "            y[review_id] = int(parts[0])  # First part is the label\n",
    "            for index, count in parse_review_features(line):\n",
    "                if index in index_mapping:\n",
    "                    X[review_id, index_mapping[index]] = int(count)\n",
    "    return X, y\n",
    "\n",
    "def generate_frequency_table(vocab_file_path, feature_path):\n",
    "    \"\"\"\n",
    "    Generates a frequency table of words in the dataset.\n",
    "    \n",
    "    Args:\n",
    "    - vocab_file_path: Path to the vocabulary file.\n",
    "    - feature_path: Path to the feature file containing word occurrences in reviews.\n",
    "    \n",
    "    Returns:\n",
    "    - A sorted list of tuples, where each tuple contains (word, frequency).\n",
    "    \"\"\"\n",
    "    index_to_word, word_to_index = load_vocab(vocab_file_path)\n",
    "    review_frequencies, total_reviews = compute_review_frequencies(feature_path)\n",
    "    word_frequencies = defaultdict(int)\n",
    "    for index, freq in review_frequencies.items():\n",
    "        word = index_to_word[index]\n",
    "        word_frequencies[word] = freq\n",
    "        \n",
    "    sorted_word_frequencies = sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_word_frequencies\n",
    "\n",
    "\n",
    "def filter_by_regression_coefficients(X, y, D=1000):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    coefficients = model.w[:-1]\n",
    "    top_indices = np.argsort(np.abs(coefficients))[-D:]\n",
    "    return top_indices, coefficients\n",
    "\n",
    "def filter_by_logistic_regression_coefficients(X, y, D=1000):\n",
    "    model = CustomLogisticRegression(learning_rate=0.01, epsilon=1e-5, max_iters=1e4)\n",
    "    model.fit(X, y)\n",
    "    coefficients = model.w[:-1]\n",
    "    top_indices = np.argsort(np.abs(coefficients))[-D:]\n",
    "    return top_indices, coefficients\n",
    "\n",
    "def extract_and_transform_multiclass_data(K=500, n_classes=5):\n",
    "    \"\"\"\n",
    "    Get the news dataset and transform it into feature vectors.\n",
    "    \n",
    "    Args:\n",
    "    - k: The number of features to select.\n",
    "    - n_classes: The number of classes.\n",
    "    \n",
    "    Returns:\n",
    "    - X_train_selected: The training feature vectors.\n",
    "    - y_train: The training labels.\n",
    "    - X_test_selected: The testing feature vectors.\n",
    "    - y_test: The testing labels.\n",
    "    \"\"\"\n",
    "    #Load the dataset\n",
    "    if n_classes == 5:\n",
    "        categories = ['comp.graphics', 'misc.forsale', 'rec.sport.baseball', 'sci.med', 'talk.politics.guns']\n",
    "        \n",
    "    elif n_classes == 10:\n",
    "        categories = ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', \n",
    "                      'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', \n",
    "                      'rec.sport.baseball']\n",
    "        \n",
    "    train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "    test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "    #Convert text to feature vectors\n",
    "    count_vect = CountVectorizer(stop_words='english', max_df=0.5, min_df=2, ngram_range=(1, 2))\n",
    "    X_train_counts = count_vect.fit_transform(train.data)\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "    #Feature selection using Mutual Information\n",
    "    feature_selection = SelectKBest(mutual_info_classif, k=K)\n",
    "    X_train_selected = feature_selection.fit_transform(X_train_tfidf, train.target)\n",
    "    X_test_counts = count_vect.transform(test.data)\n",
    "    X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "    X_test_selected = feature_selection.transform(X_test_tfidf)\n",
    "    \n",
    "    return X_train_selected, train.target, X_test_selected, test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Plotting Functions #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multiclass_classification(X_train_selected, y_train, X_test_selected, y_test, n_classes=5):\n",
    "    \"\"\"\n",
    "    Compute classification accuracy.\n",
    "    \n",
    "    Args:\n",
    "    - X_train_selected: The training feature vectors.\n",
    "    - y_train: The training labels.\n",
    "    - X_test_selected: The testing feature vectors.\n",
    "    - y_test: The testing labels.\n",
    "    - n_classes: The number of classes.\n",
    "    \n",
    "    Returns:\n",
    "    - accuracy: The classification accuracy.\n",
    "    - y_pred: The predicted labels.\n",
    "    \"\"\"\n",
    "\n",
    "    #Train the MultiClassRegression model\n",
    "    n_features = X_train_selected.shape[1]\n",
    "    mc_model = MultiClassRegression(nFeatures=n_features, nClasses=n_classes)\n",
    "    mc_model.fit(X_train_selected.toarray(), y_train, lr=0.05, niters=2000, verbose=False)\n",
    "\n",
    "    #Evaluate model\n",
    "    y_pred = mc_model.predict(X_test_selected.toarray())\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy, y_pred\n",
    "\n",
    "X_train_news, y_train_news, X_test_news, y_test_news = extract_and_transform_multiclass_data(K=500, n_classes=5)\n",
    "#accuracy, y_ped = evaluate_multiclass_classification(X_train_news, y_train_news, X_test_news, y_test_news, n_classes=5)\n",
    "#print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_top_features(coefficients, sorted_indices, index_to_feature_map, reverse_index_mapping, title, n_features=10):\n",
    "    \"\"\"\n",
    "    Plots the top N negative and positive features based on their coefficients, with a legend.\n",
    "    \n",
    "    Parameters:\n",
    "    - coefficients: Array of feature coefficients from a regression model.\n",
    "    - index_to_feature_map: Dictionary mapping original feature indices to words.\n",
    "    - reverse_index_mapping: Dictionary mapping from reduced feature space to original index.\n",
    "    - n_features: Number of top features to plot for both negative and positive coefficients.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separate positive and negative coefficients\n",
    "    positive_indices = [i for i, coef in enumerate(coefficients) if coef > 0]\n",
    "    negative_indices = [i for i, coef in enumerate(coefficients) if coef < 0]\n",
    "    \n",
    "    # Sort them by absolute values but keep the sign for color coding\n",
    "    top_positive_indices = sorted(positive_indices, key=lambda i: coefficients[i], reverse=True)[:n_features]\n",
    "    top_negative_indices = sorted(negative_indices, key=lambda i: coefficients[i])[:n_features]\n",
    "    \n",
    "    # Combine the indices and coefficients for plotting\n",
    "    top_indices = top_negative_indices + top_positive_indices\n",
    "    top_coefficients = [coefficients[i] for i in top_indices]\n",
    "    top_words = [index_to_feature_map[str(reverse_index_mapping[i])] for i in top_indices]\n",
    "    \n",
    "    # Determine colors based on coefficient sign\n",
    "    colors = ['red' if coef < 0 else 'blue' for coef in top_coefficients]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x=np.abs(top_coefficients), y=top_words, palette=colors, orient='h')\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Create a custom legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor='blue', edgecolor='blue', label='Positive'),\n",
    "                       Patch(facecolor='red', edgecolor='red', label='Negative')]\n",
    "    plt.legend(handles=legend_elements, title='Coefficient Sign')\n",
    "    plt.savefig(f'experiment_1_results_{title}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_progress(logistic_regression_model):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(logistic_regression_model.loss_history, label='Loss', color='b', ls='-',linewidth=1)\n",
    "    plt.title('Loss Progress over Iterations')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(logistic_regression_model.gradient_norm_history, label='Gradient Norm')\n",
    "    plt.title('Gradient Norm Progress over Iterations')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Gradient Norm')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('experiment_2_training_progress_results.png')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_model_convergence(histories, results):\n",
    "    \"\"\"\n",
    "    Display plots for model convergence: ROC curves, training losses, and gradient norms\n",
    "    for different learning rates.\n",
    "    \n",
    "    Args:\n",
    "    - histories: Dictionary mapping learning rates (str) to tuples of (loss_history, gradient_norm_history).\n",
    "    - results: Dictionary mapping learning rates (str) to AUROC scores.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(18, 6))\n",
    "\n",
    "    # Plot ROC curves\n",
    "    plot_multiple_roc_curves(results, 'experiment_7_roc_results.png')\n",
    "\n",
    "    # Plot training losses\n",
    "    plt.subplot(1, 3, 2)\n",
    "    for lr, (loss_history, _) in histories.items():\n",
    "        plt.plot(loss_history, label=f'LR={lr}')\n",
    "    plt.title('Training Loss Progress')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot gradient norms\n",
    "    plt.subplot(1, 3, 3)\n",
    "    for lr, (_, gradient_norm_history) in histories.items():\n",
    "        plt.plot(gradient_norm_history, label=f'LR={lr}')\n",
    "    plt.title('Gradient Norm Progress')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Gradient Norm')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('experiment_7_training_results.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_binary_classification(results):\n",
    "    \"\"\"\n",
    "    plot ROC curve\n",
    "    compare to DT from sklearn\n",
    "    \n",
    "    Args:\n",
    "    - results: tuple of fpr, tpr, auroc\n",
    "    \n",
    "    Return:\n",
    "    - None, plot the ROC curve\n",
    "    \n",
    "    \"\"\"\n",
    "    fpr, tpr, auroc = results\n",
    "    print(f\"Area under the ROC curve: {auroc}\")\n",
    "    print(f\"True positive rate: {tpr}\")\n",
    "    print(f\"False positive rate: {fpr}\")\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auroc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('experiment_4_results.png')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_multiple_roc_curves(results_dict,title):\n",
    "    \"\"\"\n",
    "    Plot ROC curves for multiple classifiers.\n",
    "    \n",
    "    Args:\n",
    "    - results_dict: Dictionary mapping model names (str) to tuples of (fpr, tpr, auroc).\n",
    "    \n",
    "    Return:\n",
    "    - None, plot the ROC curves.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Iterate through the results dictionary and plot each ROC curve\n",
    "    for model_name, (fpr, tpr, auroc) in results_dict.items():\n",
    "        plt.plot(fpr, tpr, lw=2, label=f'{model_name} ROC curve (area = {auroc:.2f})')\n",
    "    \n",
    "    # Plot diagonal line for no skill classifier\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(title)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_multiclass_classification(results):\n",
    "    \"\"\"\n",
    "    compare accuracy to DT from sklearn\n",
    "    \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def plot_top_20_features_from_imdb_simple(top_positive_features, top_negative_features):\n",
    "    \"\"\"\n",
    "    A horizontal bar plot showing the top 20 features from the Simple linear regression on the IMDB data\n",
    "    \n",
    "    Characteristics:\n",
    "    - 10 most positive and negative coefficients as the x-axis \n",
    "    - Feature names (i.e., words) as the y-axis  \n",
    "    \"\"\"\n",
    "    # Plot the top 20 positive and negative features on the same plot\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    # assign each a different color and make negatives positive\n",
    "    top_negative_features['Impact'] = top_negative_features['Impact'].abs()\n",
    "    plt.barh(top_positive_features['Word'].head(10), top_positive_features['Impact'].head(10), color='b', label='Positive Impact')\n",
    "    plt.barh(top_negative_features['Word'].head(10), top_negative_features['Impact'].head(10), color='r', label='Negative Impact')\n",
    "    plt.xlabel('Regression Coefficients (Absolute Value)')\n",
    "    plt.ylabel('Word')\n",
    "    plt.title('Top 20 Features from Simple Linear Regression')\n",
    "    plt.legend()\n",
    "    plt.savefig('top_20_features_from_imdb_simple.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_model_convergence(histories, results):\n",
    "    \"\"\"\n",
    "    Display plots for model convergence: ROC curves, training losses, and gradient norms\n",
    "    for different learning rates.\n",
    "    \n",
    "    Args:\n",
    "    - histories: Dictionary mapping learning rates (str) to tuples of (loss_history, gradient_norm_history).\n",
    "    - results: Dictionary mapping learning rates (str) to AUROC scores.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(18, 6))\n",
    "\n",
    "    # Plot training losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for lr, (loss_history, _) in histories.items():\n",
    "        plt.plot(loss_history, label=f'LR={lr}')\n",
    "    plt.title('Training Loss Progress')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot gradient norms\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for lr, (_, gradient_norm_history) in histories.items():\n",
    "        plt.plot(gradient_norm_history, label=f'LR={lr}')\n",
    "    plt.title('Gradient Norm Progress')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Gradient Norm')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig('experiment_6_results.png')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_imdb_data_auroc(results):\n",
    "    \"\"\"\n",
    "    A bar plot that shows the AUROC of logistic regression and DT on the test data (y-axis) \n",
    "    as a function of the 20%, 40%, 60%, 80%, and 100% training data (x-axis).\n",
    "    \"\"\"\n",
    "    # Extracting percentages and scores\n",
    "    percentages = list(results.keys())\n",
    "    accuracy_mc = [result[0] for result in results.values()]\n",
    "    accuracy_dt = [result[1] for result in results.values()]\n",
    "    \n",
    "    # Setting up the bar plot with an adjusted figure size\n",
    "    plt.figure(figsize=(12, 8))  # Adjust figure size as needed\n",
    "    ax = plt.subplot(111)\n",
    "    index = np.arange(len(results))\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.8\n",
    "    \n",
    "    bars1 = ax.bar(index, accuracy_mc, bar_width, alpha=opacity, color='b', label='Logistic Regression')\n",
    "    bars2 = ax.bar(index + bar_width, accuracy_dt, bar_width, alpha=opacity, color='g', label='Decision Tree')\n",
    "    \n",
    "    # Adding labels, title, and axes ticks\n",
    "    ax.set_xlabel('Training Set Size (%)')\n",
    "    ax.set_ylabel('AUROC')\n",
    "    ax.set_title('AUROC by Model and Training Set Size')\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels(percentages)\n",
    "    \n",
    "    # Adjusting the legend position to avoid overlap and ensure it's inside the plot area\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
    "    \n",
    "    # Add labels onto the bars with some adjustments for better readability\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(f'{height:.2f}',\n",
    "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom')\n",
    "    \n",
    "    # Adjust layout to make room for the legend and ensure labels are not cut off\n",
    "    plt.subplots_adjust(right=0.75)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('experiment_5_results.png')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_word_distribution(review_frequencies, title):\n",
    "    \"\"\"\n",
    "    Plots a histogram and distribution of word counts in a dataset.\n",
    "    Args:\n",
    "    - data: Tuple containing (review_frequencies, total_reviews), where review_frequencies\n",
    "      is a dictionary mapping word indices to frequencies, and total_reviews is the total number of reviews.\n",
    "    - title: Title for the plot.\n",
    "    \"\"\"\n",
    "    indices = np.array([int(index) for index in review_frequencies.keys()])\n",
    "    frequencies = np.array(list(review_frequencies.values()))\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(indices, weights=frequencies, bins=100, color='skyblue', edgecolor='black')\n",
    "    plt.title(f'{title} - Histogram')\n",
    "    plt.xlabel('Word Index')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.yscale('log', nonpositive='clip')\n",
    "    plt.grid(True, which=\"both\", ls=\"--\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    kde = gaussian_kde(indices, weights=frequencies)\n",
    "    idx_range = np.linspace(indices.min(), indices.max(), 300)\n",
    "    plt.plot(idx_range, kde(idx_range), color='darkblue', lw=2)\n",
    "    plt.title(f'{title} - Density Plot')\n",
    "    plt.xlabel('Word Index')\n",
    "    plt.ylabel('Density')\n",
    "    plt.yscale('log', nonpositive='clip')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'imdb_dataset_word_distribution.png')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_news_data_classification_accuracy(results):\n",
    "    \"\"\"\n",
    "    Plot the classification accuracies of mutliclass regression, DT, and KNN on the test data.\n",
    "    \n",
    "    Args:\n",
    "    - results: Dictionary mapping model names (str) to classification accuracies.\n",
    "    \"\"\"\n",
    "    # Extracting model names and scores\n",
    "    model_names = list(results.keys())\n",
    "    accuracies = list(results.values())\n",
    "    \n",
    "    # Setting up the bar plot with an adjusted figure size\n",
    "    plt.figure(figsize=(12, 8))  # Adjust figure size as needed\n",
    "    ax = plt.subplot(111)\n",
    "    index = np.arange(len(results))\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.8\n",
    "    \n",
    "    bars = ax.bar(index, accuracies, bar_width, alpha=opacity, color='b')\n",
    "    \n",
    "    # Adding labels, title, and axes ticks\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Classification Accuracy by Model')\n",
    "    ax.set_xticks(index)\n",
    "    ax.set_xticklabels(model_names)\n",
    "    \n",
    "    # Add labels onto the bars with some adjustments for better readability\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "    \n",
    "    # Adjust layout to ensure labels are not cut off\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('experiment_2_newsdata_results.png')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_news_data_on_multiple_training_sizes(results):\n",
    "    \"\"\"\n",
    "    A bar plot that shows the classification accuracies of multiclass regression and DT \n",
    "    on the test data (y-axis) as a function of the 20%, 40%, 60%, 80%, and 100% training data (x- axis)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extracting percentages and scores\n",
    "    percentages = list(results.keys())\n",
    "    accuracy_mc = [result[0] for result in results.values()]  # Multiclass regression accuracies\n",
    "    accuracy_dt = [result[1] for result in results.values()]  # Decision tree accuracies\n",
    "    \n",
    "    # Setting up the bar plot with an adjusted figure size\n",
    "    plt.figure(figsize=(12, 8))  # Adjust figure size as needed\n",
    "    ax = plt.subplot(111)\n",
    "    index = np.arange(len(results))\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.8\n",
    "    \n",
    "    bars1 = ax.bar(index, accuracy_mc, bar_width, alpha=opacity, color='b', label='MC Regression')\n",
    "    bars2 = ax.bar(index + bar_width, accuracy_dt, bar_width, alpha=opacity, color='g', label='Decision Tree')\n",
    "    \n",
    "    # Adding labels, title, and axes ticks\n",
    "    ax.set_xlabel('Training Set Size (%)')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Accuracy by Model and Training Set Size')\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels(percentages)\n",
    "    \n",
    "    # Adjusting the legend position to avoid overlap and ensure it's inside the plot area\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
    "    \n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(f'{height:.2f}',\n",
    "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom')\n",
    "    \n",
    "    # Adjust layout to make room for the legend and ensure labels are not cut off\n",
    "    plt.subplots_adjust(right=0.75)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('experiment_4_results.png')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_heatmap_of_multi_classification(results):\n",
    "    \"\"\"\n",
    "    A heatmap showing the top 5 most positive features as rows for each class as columns in the multi-class classification \n",
    "    on 4 the chosen classes from the 20-news group datasets. Therefore, your heatmap should be a 20-by-4 dimension.\n",
    "\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Driver code to load everything\n",
    "\n",
    "# Define globals\n",
    "VOCAB_FILE = '../aclImdb/imdb.vocab'\n",
    "TRAIN_FEAT = '../aclImdb/train/labeledBow.feat'\n",
    "TEST_FEAT = '../aclImdb/test/labeledBow.feat'\n",
    "N_REVIEWS = 25000\n",
    "\n",
    "# load the data and filter the reviews\n",
    "index_to_feature_map, feature_to_index_map = load_vocab(VOCAB_FILE)\n",
    "review_frequencies, total_reviews = compute_review_frequencies(TRAIN_FEAT)\n",
    "filtered_indices = filter_feature_indices(review_frequencies, total_reviews)\n",
    "index_mapping = {str(old_index): new_index for new_index, old_index in enumerate(filtered_indices)}\n",
    "\n",
    "# plot the distribution\n",
    "plot_word_distribution(review_frequencies, \"Distribution of Words in the IMDB Dataset\")\n",
    "\n",
    "# create train and test based on the filtered indices\n",
    "N_FEATURES = len(filtered_indices)\n",
    "X_train, y_train = load_review_features(TRAIN_FEAT, index_mapping, N_REVIEWS, N_FEATURES)\n",
    "X_test, y_test = load_review_features(TEST_FEAT, index_mapping, N_REVIEWS, N_FEATURES)\n",
    "\n",
    "# Now filter by the regression coefficients\n",
    "top_indices, coefficients = filter_by_regression_coefficients(X_train, y_train)\n",
    "reverse_index_mapping = {new_index: old_index for old_index, new_index in index_mapping.items()}\n",
    "\n",
    "# Convert y_train and y_test to binary\n",
    "y_train_binary = (y_train > 5).astype(int)\n",
    "y_test_binary = (y_test > 5).astype(int)\n",
    "\n",
    "# Filter by regression coefficients from logistic regression\n",
    "#top_log_indices, top_log_coefficients = filter_by_logistic_regression_coefficients(X_train, y_train_binary)\n",
    "#reverse_index_mapping = {new_index: old_index for old_index, new_index in index_mapping.items()}\n",
    "\n",
    "# Filter features based on top_indices\n",
    "X_train_filtered = X_train[:, top_indices]\n",
    "X_test_filtered = X_test[:, top_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_one():\n",
    "    \"\"\"\n",
    "    Report the:\n",
    "    - top 10 features with the most positive coefficients\n",
    "    - top 10 features with the most negative coefficients \n",
    "    \n",
    "    on the IMDB data using simple linear regression on the movie rating scores\n",
    "    \"\"\"\n",
    "    title1 = 'Top 20 Features by Absolute Coefficient Value From Linear Regression'\n",
    "    title2 = 'Top 20 Features by Absolute Coefficient Value From Logistic Regression'\n",
    "    plot_top_features(coefficients, top_indices[::-1], index_to_feature_map, reverse_index_mapping, title1, n_features=10)\n",
    "    plot_top_features(top_log_coefficients, top_log_indices[::-1], index_to_feature_map, reverse_index_mapping, title2, n_features=10)\n",
    "    \n",
    "\n",
    "#experiment_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_two(X_train, y_train, X_test, y_test, top_indices):\n",
    "    \"\"\"\n",
    "    Implement and conduct Binary classification on the IMDb Reviews\n",
    "    \"\"\"\n",
    "    \n",
    "    model = CustomLogisticRegression(learning_rate=0.01, epsilon=1e-5, max_iters=1e4,record_training=True)\n",
    "    model.fit(X_train_filtered, y_train_binary)\n",
    "    plot_training_progress(model)\n",
    "    y_pred_prob_test = model.predict(X_test_filtered)\n",
    "    y_pred_prob_train = model.predict(X_train_filtered)\n",
    "    \n",
    "    # Compute ROC curve and ROC area for the test set\n",
    "    fpr_test, tpr_test, _ = roc_curve(y_test_binary, y_pred_prob_test)\n",
    "    roc_auc_test = auc(fpr_test, tpr_test)\n",
    "    \n",
    "    # Compute ROC curve and ROC area for the training set\n",
    "    fpr_train, tpr_train, _ = roc_curve(y_train_binary, y_pred_prob_train)\n",
    "    roc_auc_train = auc(fpr_train, tpr_train)\n",
    "    results = {\"Logisitic Regression (Train)\": (fpr_train, tpr_train, roc_auc_train), \n",
    "               \"Logisitic Regression (Test)\": (fpr_test, tpr_test, roc_auc_test)}\n",
    "    plot_multiple_roc_curves(results, 'experiment_2_results.png')\n",
    "    \n",
    "#experiment_two(X_train_filtered, y_train_binary, X_test_filtered, y_test_binary, top_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_two_multiclass(X_train_selected, y_train, X_test_selected, y_test, n_classes=5):\n",
    "    \"\"\"\n",
    "    Plot the multiclass classification accuracy of multiclass linear regression and Decision Trees \n",
    "    on the 5 chosen classes from the 20-news-group data\n",
    "    \"\"\"\n",
    "    multiclass_accuracy, y_pred = evaluate_multiclass_classification(X_train_selected, y_train, X_test_selected, y_test, n_classes)\n",
    "    \n",
    "    # Train a Decision Tree model\n",
    "    dt_model = DecisionTreeClassifier()\n",
    "    dt_model.fit(X_train_selected, y_train) \n",
    "    y_pred_dt = dt_model.predict(X_test_selected)\n",
    "    accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "    \n",
    "    # Train a KNN model\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn_model.fit(X_train_selected, y_train)\n",
    "    y_pred_knn = knn_model.predict(X_test_selected)\n",
    "    accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "    \n",
    "    # Plot the results\n",
    "    results = {\"Multiclass Regression\": multiclass_accuracy, \"Decision Tree\": accuracy_dt, \"KNN\": accuracy_knn}\n",
    "    plot_news_data_classification_accuracy(results)\n",
    "    \n",
    "experiment_two_multiclass(X_train_news, y_train_news, X_test_news, y_test_news, n_classes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_three(X_train, y_train, X_test, y_test, top_indices):\n",
    "    \"\"\"\n",
    "    On the same plot as 2, draw ROC curves and report the AUROC \n",
    "    values of logistic regression and Decision Trees on the \n",
    "    IMDB data binary classification task\n",
    "    \"\"\"\n",
    "    \n",
    "    models = [CustomLogisticRegression(learning_rate=0.01, epsilon=1e-5, max_iters=1e4), \n",
    "              DecisionTreeClassifier(), \n",
    "              KNeighborsClassifier(), \n",
    "              LR(C=1/0.01, fit_intercept=True, max_iter=int(1e4), tol=1e-5)] #sklearn logistic regression\n",
    "    results = {}\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_prob_test = model.predict(X_test)\n",
    "        y_pred_prob_train = model.predict(X_train)\n",
    "\n",
    "        # Compute ROC curve and ROC area for the test set\n",
    "        fpr_test, tpr_test, _ = roc_curve(y_test, y_pred_prob_test)\n",
    "        roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "        # Compute ROC curve and ROC area for the training set\n",
    "        fpr_train, tpr_train, _ = roc_curve(y_train, y_pred_prob_train)\n",
    "        roc_auc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "        #results[f'{model.__class__.__name__} (train)'] = (fpr_train, tpr_train, roc_auc_train)\n",
    "        results[f'{model.__class__.__name__} (test)'] = (fpr_test, tpr_test, roc_auc_test)\n",
    "        \n",
    "    plot_multiple_roc_curves(results,title='experiment_3_results.png')\n",
    "    \n",
    "#experiment_three(X_train_filtered, y_train_binary, X_test_filtered, y_test_binary, top_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_four(X_train_news, y_train_news, X_test_news, y_test_news):\n",
    "    \"\"\"\n",
    "    Plot and compare the accuracy of DT and Multiclass Regression as a function of the\n",
    "    size of the dataset by controlling the training size.\n",
    "    \n",
    "    Randomly select 20%, 40%, 60%, and 80% of the available \n",
    "    training data and train your model on this subset and evaluate the trained \n",
    "    model on the held-out test set.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for i in range(20, 101, 20):\n",
    "        train_size = i/100\n",
    "        if i < 100:\n",
    "            _, X_train_subset, _, y_train_subset = train_test_split(X_train_news, y_train_news, train_size=train_size, stratify=y_train_news)\n",
    "        else:  \n",
    "            X_train_subset = X_train_news\n",
    "            y_train_subset = y_train_news\n",
    "            train_size = 1\n",
    "        \n",
    "        multiclass_accuracy, y_pred = evaluate_multiclass_classification(X_train_subset, y_train_subset, X_test_news, y_test_news, n_classes=5)\n",
    "        \n",
    "        dt_model = DecisionTreeClassifier()\n",
    "        dt_model.fit(X_train_subset, y_train_subset)\n",
    "        y_pred_dt = dt_model.predict(X_test_news)\n",
    "        accuracy_dt = accuracy_score(y_test_news, y_pred_dt)\n",
    "        \n",
    "        results[f'{train_size*100}%'] = (multiclass_accuracy, accuracy_dt)\n",
    "        \n",
    "    plot_news_data_on_multiple_training_sizes(results)\n",
    "    \n",
    "experiment_four(X_train_news, y_train_news, X_test_news, y_test_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_five(X_train, y_train, X_test, y_test, top_indices):\n",
    "    \"\"\"\n",
    "    Plot and compare the AUROC of DT and LogisticRegression as a function of the\n",
    "    size of the dataset by controlling the training size.\n",
    "    \n",
    "    Randomly select 20%, 40%, 60%, and 80% of the available \n",
    "    training data and train your model on this subset and evaluate the trained \n",
    "    model on the held-out test set.\n",
    "    \"\"\"\n",
    "    log_model = CustomLogisticRegression(learning_rate=0.01, epsilon=1e-5, max_iters=1e4)\n",
    "    dt_model = DecisionTreeClassifier()\n",
    "    results = {}  # Ex: '20%': (auroc_log, auroc_dt)\n",
    "    \n",
    "    for i in range(20, 101, 20):\n",
    "        if i < 100:\n",
    "            _, X_train_sub, _, y_train_sub = train_test_split(X_train, y_train, train_size=i/100, stratify=y_train)\n",
    "        else:\n",
    "            X_train_sub = X_train\n",
    "            y_train_sub = y_train\n",
    "        \n",
    "        log_model.fit(X_train_sub, y_train_sub)\n",
    "        dt_model.fit(X_train_sub, y_train_sub)\n",
    "    \n",
    "        y_pred_prob_test_log = log_model.predict(X_test)\n",
    "        y_pred_prob_test_dt = dt_model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        auroc_log = auc(*roc_curve(y_test, y_pred_prob_test_log)[:2])\n",
    "        auroc_dt = auc(*roc_curve(y_test, y_pred_prob_test_dt)[:2])\n",
    "        \n",
    "        results[f'{i}%'] = (auroc_log, auroc_dt)\n",
    "    \n",
    "    # Plotting the results\n",
    "    plot_imdb_data_auroc(results)\n",
    "    \n",
    "#experiment_five(X_train_filtered, y_train_binary, X_test_filtered, y_test_binary, top_indices)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 6 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_six(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Compare and evaluate the performance of different learning rates\n",
    "    \"\"\"\n",
    "    histories = {}\n",
    "    results = {}\n",
    "    for i in range(1,5):\n",
    "        lr = 10**-i\n",
    "        model = CustomLogisticRegression(learning_rate=lr, epsilon=1e-5, max_iters=1e4, record_training=True)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_prob = model.predict(X_test)\n",
    "        \n",
    "        histories[str(lr)] = (model.loss_history, model.gradient_norm_history)\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "        roc_auc_train = auc(fpr, tpr)\n",
    "        results[str(lr)] = (fpr, tpr, roc_auc_train)\n",
    "        \n",
    "    plot_model_convergence(histories, results)\n",
    "        \n",
    "#experiment_six(X_train_filtered, y_train_binary, X_test_filtered, y_test_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 7 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_seven(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of the multiclass regression on more than 5 classes\n",
    "    \n",
    "    Compare the top k (e.g. k =3) predicted classes. \n",
    "    A correct prediction is determined by whether the true label is within the top k predicted labels. \n",
    "    The scoring mechanism involves assigning a score of 1 if the correct label is among the top k predictions and 0 otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
