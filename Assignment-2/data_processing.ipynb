{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import os\n",
    "from LinearRegression import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "#from LogisticRegression import LogisticRegression as LR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from MultiClassRegression import MultiClassRegression\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extact, Transform, Load (ETL) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATINGS = [1, 2, 3, 4, 7, 8, 9, 10]\n",
    "\n",
    "def extract_word_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read().splitlines()\n",
    "\n",
    "def parse_features_and_labels(feature_path='../aclImdb/test/labeledBow.feat'):\n",
    "    \"\"\"\n",
    "    Parse the feature file to extract word frequencies and labels for each review.\n",
    "    \n",
    "    Args:\n",
    "    - feature_path (str): Path to the feature file.\n",
    "    \n",
    "    Returns:\n",
    "    - List of dictionaries with 'label' and 'features' keys.\n",
    "    \"\"\"\n",
    "    reviews_data = []\n",
    "    with open(feature_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            label, *features = line.strip().split() # ratings and (word : count) pairs\n",
    "            review_data = {\n",
    "                'label': int(label),\n",
    "                'features': {int(f.split(':')[0]): int(f.split(':')[1]) for f in features}\n",
    "            }\n",
    "            reviews_data.append(review_data)\n",
    "    return reviews_data\n",
    "\n",
    "def create_frequency_dataframe(reviews_data):\n",
    "    \"\"\"\n",
    "    Create a DataFrame from review data with word frequencies for each label.\n",
    "    \n",
    "    Args:\n",
    "    - reviews_data (List[Dict]): Parsed review data including labels and features.\n",
    "    \n",
    "    Returns:\n",
    "    - A pandas DataFrame with word indices as rows, labels as columns, and frequencies as cell values.\n",
    "    \"\"\"\n",
    "    word_label_freq = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for review in reviews_data:\n",
    "        label = review['label']\n",
    "        for word_index, count in review['features'].items():\n",
    "            word_label_freq[word_index][label] += count\n",
    "            \n",
    "    data = []\n",
    "    for word_index, label_freqs in word_label_freq.items():\n",
    "        \n",
    "        for label, freq in label_freqs.items():\n",
    "            data.append({'word_index': word_index, 'label': label, 'frequency': freq})\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df_pivoted = df.pivot(index='word_index', columns='label', values='frequency').fillna(0)\n",
    "\n",
    "    for rating in RATINGS:\n",
    "        if rating not in df_pivoted.columns:\n",
    "            df_pivoted[rating] = 0.0\n",
    "    df_pivoted = df_pivoted[RATINGS]\n",
    "\n",
    "    return df_pivoted\n",
    "\n",
    "def add_words_to_dataframe(df, word_data_path='../aclImdb/imdb.vocab', debug=False):\n",
    "    \"\"\"\n",
    "    Add words to the DataFrame as a new column, matching the index of each word in the vocab file\n",
    "    to the word index in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    - df (pd.DataFrame): DataFrame with word frequencies for each label.\n",
    "    - word_data_path (str): Path to the vocab file.\n",
    "    \n",
    "    Returns:\n",
    "    - A DataFrame with a new 'word' column.\n",
    "    \"\"\"\n",
    "    words = extract_word_data(word_data_path)\n",
    "    \n",
    "    # The vocabulary indices are off by one, subtract one to align with the DataFrame\n",
    "    index_to_word = {index: word for index, word in enumerate(words, start=0)}\n",
    "    \n",
    "    # Map the word indices to the DataFrame indices\n",
    "    df['word'] = df.index.map(index_to_word)\n",
    "\n",
    "    if debug:\n",
    "        # Verify all DataFrame indices have corresponding words in vocab\n",
    "        vocab_indices_set = set(range(len(words)))\n",
    "        dataframe_indices_set = set(df.index.astype(int))\n",
    "        \n",
    "        missing_indices_in_vocab = dataframe_indices_set - vocab_indices_set\n",
    "        if missing_indices_in_vocab:\n",
    "            print(f\"Warning: {len(missing_indices_in_vocab)} indices in DataFrame not found in vocab file.\")\n",
    "            \n",
    "        missing_indices_in_df = vocab_indices_set - dataframe_indices_set\n",
    "        if missing_indices_in_df:\n",
    "            print(f\"Note: {len(missing_indices_in_df)} indices in vocab file not represented in DataFrame. This may be expected due to filtering.\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_features(reviews_data, min_feature_frequency=0.01, max_feature_frequency=0.5, use_quantiles=False, quantiles=(0.05, 0.95)):\n",
    "    \"\"\"\n",
    "    Filters out features (words) that occur in less than the specified minimum frequency of reviews or more than the specified maximum frequency of reviews.\n",
    "    Optionally, filters based on quantiles.\n",
    "\n",
    "    Args:\n",
    "    - reviews_data (List[Dict]): List of dictionaries, each representing a review with word frequencies.\n",
    "    - min_feature_frequency (float): Minimum frequency threshold as a fraction of total reviews if not using quantiles.\n",
    "    - max_feature_frequency (float): Maximum frequency threshold as a fraction of total reviews if not using quantiles.\n",
    "    - use_quantiles (bool): Whether to use quantile-based filtering.\n",
    "    - quantiles (tuple): A tuple representing the lower and upper quantile thresholds for filtering.\n",
    "\n",
    "    Returns:\n",
    "    - A filtered list of dictionaries, with each dictionary representing a review and containing only the features that meet the frequency criteria.\n",
    "    \"\"\"\n",
    "    total_reviews = len(reviews_data)\n",
    "    word_occurrences = defaultdict(int)\n",
    "\n",
    "    # Count the number of reviews each word appears in\n",
    "    for review in reviews_data:\n",
    "        for word_index in review['features']:\n",
    "            word_occurrences[word_index] += 1\n",
    "\n",
    "    if use_quantiles:\n",
    "        # Use quantile-based filtering\n",
    "        sorted_occurrences = sorted(word_occurrences.values())\n",
    "        lower_bound = sorted_occurrences[int(quantiles[0] * total_reviews)]\n",
    "        upper_bound = sorted_occurrences[int(quantiles[1] * total_reviews) - 1]\n",
    "    else:\n",
    "        # Use fixed frequency thresholds\n",
    "        lower_bound = total_reviews * min_feature_frequency\n",
    "        upper_bound = total_reviews * max_feature_frequency\n",
    "\n",
    "    # Identify words that meet the frequency criteria\n",
    "    valid_words = {word_index for word_index, count in word_occurrences.items()\n",
    "                   if lower_bound <= count <= upper_bound}\n",
    "\n",
    "    # Filter reviews to only include valid words\n",
    "    filtered_reviews_data = []\n",
    "    for review in reviews_data:\n",
    "        filtered_features = {word_index: count for word_index, count in review['features'].items()\n",
    "                             if word_index in valid_words}\n",
    "        filtered_review = {'label': review['label'], 'features': filtered_features}\n",
    "        filtered_reviews_data.append(filtered_review)\n",
    "\n",
    "    return filtered_reviews_data\n",
    "\n",
    "def select_top_features_by_regression_coefficients(df, n=500):\n",
    "    features_df = df.drop(columns=['word'])\n",
    "    top_features = []\n",
    "    correlations = []\n",
    "    for index, row in features_df.iterrows():\n",
    "        word = df.loc[index, 'word']\n",
    "        \n",
    "        X = row.values.reshape(-1, 1)\n",
    "        y = np.array(RATINGS).reshape(-1, 1)\n",
    "        model = LR()\n",
    "        model.fit(X, y)\n",
    "        coefficient = model.coef_[0][0]  # Assuming a single coefficient\n",
    "        top_features.append((word, coefficient))\n",
    "        correlation = np.corrcoef(X.flatten(), y.flatten())[0, 1]\n",
    "        correlations.append((word, correlation))\n",
    "    \n",
    "    top_features.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    top_n_features = [feature[0] for feature in top_features[:n]]\n",
    "    top_features_df = pd.DataFrame(top_features[:n], columns=['word', 'coefficient'])\n",
    "    top_correlations_df = pd.DataFrame(correlations[:n], columns=['word', 'correlation'])\n",
    "    \n",
    "    return df[df['word'].isin(top_n_features)], top_features_df, top_correlations_df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the reviews data (features and labels)\n",
    "reviews_data = parse_features_and_labels()\n",
    "# Filter the features by frequency\n",
    "filtered_reviews_data = filter_features(reviews_data, min_feature_frequency=0.01, max_feature_frequency=0.5)\n",
    "# Create a DataFrame with word frequencies for each label (rating)\n",
    "imdb_feature_frequency_df = create_frequency_dataframe(filtered_reviews_data)\n",
    "unfiltered_imdb_feature_frequency_df = create_frequency_dataframe(reviews_data)\n",
    "unfiltered_imdb_feature_frequency_df = add_words_to_dataframe(unfiltered_imdb_feature_frequency_df, debug=True)\n",
    "print(imdb_feature_frequency_df.head())\n",
    "imdb_feature_frequency_df = add_words_to_dataframe(imdb_feature_frequency_df, debug=True)\n",
    "top_features, features_and_coefficients, correlations = select_top_features_by_regression_coefficients(imdb_feature_frequency_df, n=500)\n",
    "print(features_and_coefficients.head())\n",
    "print(top_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Plotting Functions #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_multiclass_classification(df):\n",
    "    \"\"\"\n",
    "    compute classification accuracy\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_frequency_distribution_with_regression(df, word_of_interest):\n",
    "    \"\"\"\n",
    "    Plot the distribution of frequencies for a specific word across labels with flipped axes and a regression line.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): DataFrame with word frequencies for each label and a 'word' column.\n",
    "    - word_of_interest (str): The word to plot the distribution for.\n",
    "    - RATINGS (list): The list of labels (ratings) corresponding to the columns in df.\n",
    "    \"\"\"\n",
    "    # Ensure the word of interest is in the DataFrame\n",
    "    if word_of_interest not in df['word'].values:\n",
    "        print(f\"The word '{word_of_interest}' does not exist in the DataFrame.\")\n",
    "        return\n",
    "\n",
    "    # Extract frequencies for the word of interest\n",
    "    word_data = df.loc[df['word'] == word_of_interest, RATINGS].values.flatten()\n",
    "    \n",
    "    # Convert RATINGS to a numeric scale if they're not already (e.g., strings to integers)\n",
    "    ratings_numeric = np.arange(len(RATINGS))\n",
    "\n",
    "    # Prepare data for regression model\n",
    "    X = word_data.reshape(-1, 1)  # Frequency data as features\n",
    "    y = ratings_numeric.reshape(-1, 1)  # Numeric ratings as target\n",
    "\n",
    "    # Fit the linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Predict y values for plotting the regression line\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X, y, color='blue', label=f'Frequencies of \"{word_of_interest}\"')\n",
    "    plt.plot(X, y_pred, color='red', label='Regression Line')\n",
    "    plt.title(f'Distribution of Frequencies for \"{word_of_interest}\" Across Labels')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Labels (Ratings)')\n",
    "    plt.yticks(ratings_numeric, RATINGS)  # Map numeric ratings back to original RATINGS for y-axis labels\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_word_distribution(df, column='frequency', include_percentiles=False):\n",
    "    \"\"\"\n",
    "    Plot the distribution of word frequencies using Seaborn and optionally mark specified percentiles.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): DataFrame containing word frequencies.\n",
    "    - column (str): The column name in df that contains the word frequencies.\n",
    "    - include_percentiles (bool): Whether to include and annotate percentiles on the plot.\n",
    "    \"\"\"\n",
    "    # Setting the style of seaborn\n",
    "    sns.set(style='whitegrid')\n",
    "\n",
    "    # Creating the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df[column], bins=30, kde=True, color='skyblue')\n",
    "\n",
    "    if include_percentiles:\n",
    "        # Specified percentiles to include\n",
    "        percentiles = [1, 2, 5, 25, 50, 75]\n",
    "        percentile_values = np.percentile(df[column], percentiles)\n",
    "\n",
    "        # Annotate each specified percentile on the plot\n",
    "        for percentile, value in zip(percentiles, percentile_values):\n",
    "            plt.axvline(x=value, color='red', linestyle='--')\n",
    "            plt.text(value, plt.gca().get_ylim()[1], f'{value:.2f}', color='red', ha='right', va='bottom')\n",
    "            \n",
    "            # Calculate and print the number of words below each percentile threshold\n",
    "            num_words_below = (df[column] <= value).sum()\n",
    "            print(f'Number of words below the {percentile}th percentile (frequency <= {value:.2f}): {num_words_below}')\n",
    "            print(f'Percentage of total words below this percentile: {100 * num_words_below / 89526:.2f}%')\n",
    "\n",
    "        plt.title('Distribution of Word Frequencies with Percentiles')\n",
    "    else:\n",
    "        plt.title('Distribution of Word Frequencies')\n",
    "    \n",
    "    plt.xlabel('Word Frequency')\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum frequencies across labels to get a total frequency for each word\n",
    "#imdb_feature_frequency_df['total_frequency'] = imdb_feature_frequency_df.sum(axis=1)\n",
    "plot_word_frequency_distribution_with_regression(imdb_feature_frequency_df, 'avoid')\n",
    "# Now you can plot the distribution of these total frequencies\n",
    "#plot_word_distribution(imdb_feature_frequency_df, column='total_frequency', include_percentiles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_training_progress(logistic_regression_model):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(logistic_regression_model.loss_history, label='Loss', color='b', ls='-',linewidth=1)\n",
    "    plt.title('Loss Progress over Iterations')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(logistic_regression_model.gradient_norm_history, label='Gradient Norm')\n",
    "    plt.title('Gradient Norm Progress over Iterations')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Gradient Norm')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_binary_classification(results):\n",
    "    \"\"\"\n",
    "    plot ROC curve\n",
    "    compare to DT from sklearn\n",
    "    \n",
    "    Args:\n",
    "    - results: tuple of fpr, tpr, auroc\n",
    "    \n",
    "    Return:\n",
    "    - None, plot the ROC curve\n",
    "    \n",
    "    \"\"\"\n",
    "    fpr, tpr, auroc = results\n",
    "    print(f\"Area under the ROC curve: {auroc}\")\n",
    "    print(f\"True positive rate: {tpr}\")\n",
    "    print(f\"False positive rate: {fpr}\")\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auroc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_multiclass_classification(results):\n",
    "    \"\"\"\n",
    "    compare accuracy to DT from sklearn\n",
    "    \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def plot_top_20_features_from_imdb_simple(top_positive_features, top_negative_features):\n",
    "    \"\"\"\n",
    "    A horizontal bar plot showing the top 20 features from the Simple linear regression on the IMDB data\n",
    "    \n",
    "    Characteristics:\n",
    "    - 10 most positive and negative coefficients as the x-axis \n",
    "    - Feature names (i.e., words) as the y-axis  \n",
    "    \"\"\"\n",
    "    # Plot the top 20 positive and negative features on the same plot\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    # assign each a different color and make negatives positive\n",
    "    top_negative_features['Impact'] = top_negative_features['Impact'].abs()\n",
    "    plt.barh(top_positive_features['Word'].head(10), top_positive_features['Impact'].head(10), color='b', label='Positive Impact')\n",
    "    plt.barh(top_negative_features['Word'].head(10), top_negative_features['Impact'].head(10), color='r', label='Negative Impact')\n",
    "    plt.xlabel('Regression Coefficients (Absolute Value)')\n",
    "    plt.ylabel('Word')\n",
    "    plt.title('Top 20 Features from Simple Linear Regression')\n",
    "    plt.legend()\n",
    "    #plt.savefig('top_20_features_from_imdb_simple.png')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_model_convergence(results, learning_rate):\n",
    "    \"\"\"\n",
    "    Convergence plot on how the logistic and multiclass regression converge given a reason- ably chosen learning rate.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def plot_imdb_data_roc_curve(results):\n",
    "    \"\"\"\n",
    "    A single plot containing two ROC curves of logistic regression and sklearn-DT (Decision Trees) on the IMDB test data.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def plot_imdb_data_auroc(results):\n",
    "    \"\"\"\n",
    "    A bar plot that shows the AUROC of logistic regression and DT on the test data (y-axis) \n",
    "    as a function of the 20%, 40%, 60%, 80%, and 100% training data (x-axis)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def plot_news_data_classification_accuracy(results):\n",
    "    \"\"\"\n",
    "    A bar plot that shows the classification accuracies of multiclass regression and DT \n",
    "    on the test data (y-axis) as a function of the 20%, 40%, 60%, 80%, and 100% training data (x- axis)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def plot_top_20_features_from_imdb_logistic(results):\n",
    "    \"\"\"\n",
    "    A horizontal bar plot showing the top 20 features (10 most positive and 10 most negative) \n",
    "    from the logistic regression on the IMDB data with the coefficient as the x-axis and the feature names (i.e., words) as the y-axis\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def plot_heatmap_of_multi_classification(results):\n",
    "    \"\"\"\n",
    "    A heatmap showing the top 5 most positive features as rows for each class as columns in the multi-class classification \n",
    "    on 4 the chosen classes from the 20-news group datasets. Therefore, your heatmap should be a 20-by-4 dimension.\n",
    "\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_progress(model)\n",
    "plot_binary_classification(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_one():\n",
    "    \"\"\"\n",
    "    Report the:\n",
    "    - top 10 features with the most positive coefficients\n",
    "    - top 10 features with the most negative coefficients \n",
    "    \n",
    "    on the IMDB data using simple linear regression on the movie rating scores\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_two():\n",
    "    \"\"\"\n",
    "    Implement and conduct:\n",
    "    - Binary classification on the IMDb Reviews\n",
    "    - Multi-class classification on the 20 news group dataset\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_three():\n",
    "    \"\"\"\n",
    "    On the same plot as 2, draw ROC curves and report the AUROC \n",
    "    values of logistic regression and Decision Trees on the \n",
    "    IMDB data binary classification task\n",
    "    \n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_four():\n",
    "    \"\"\"\n",
    "    Report the:\n",
    "    - Multiclass classification accuracy of multiclass linear regression \n",
    "    - Decision Trees on the 5 chosen classes from the 20-news-group data\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_five():\n",
    "    \"\"\"\n",
    "    Plot and compare the accuracy of the two models as a function of the\n",
    "    size of dataset by controlling the training size\n",
    "    \n",
    "    For example, you can randomly select 20%, 40%, 60% and 80% of the available \n",
    "    training data and train your model on this subset and evaluate the trained \n",
    "    model on the held-out test set.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 6 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_six():\n",
    "    \"\"\"\n",
    "    Compare and evaluate the performance of different learning rates\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 7 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_seven():\n",
    "    \"\"\"\n",
    "    Evaluate the performance of the multiclass regression on more than 5 classes\n",
    "    \n",
    "    Compare the top k (e.g. k =3) predicted classes. \n",
    "    A correct prediction is determined by whether the true label is within the top k predicted labels. \n",
    "    The scoring mechanism involves assigning a score of 1 if the correct label is among the top k predictions and 0 otherwise.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
